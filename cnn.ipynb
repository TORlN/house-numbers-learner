{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import sys\n",
    "import preprocess\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_32x32.mat found\n",
      "./test_32x32.mat found\n",
      "(32, 32, 3, 73257)\n",
      "(73257, 1)\n"
     ]
    }
   ],
   "source": [
    "    # Adjust file paths as needed for your local file system\n",
    "    file_path = './train_32x32.mat'\n",
    "\n",
    "    # Check if file is in your specified local directory\n",
    "    if not os.path.exists(file_path):\n",
    "        print(file_path,\" not found, exiting\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print(file_path, \"found\")\n",
    "\n",
    "    # Load the training data\n",
    "    train_mat = scipy.io.loadmat(file_path)\n",
    "\n",
    "    # categorize the data\n",
    "    X_tr = train_mat['X']\n",
    "    y_tr = train_mat['y']\n",
    "    y_tr = y_tr.astype(int)\n",
    "\n",
    "    file_path = \"./test_32x32.mat\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(file_path,\" not found, exiting\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print(file_path, \"found\")\n",
    "\n",
    "    # Load the test data\n",
    "    test_mat = scipy.io.loadmat(file_path)\n",
    "\n",
    "    X_te = test_mat['X']\n",
    "    y_te = test_mat['y']\n",
    "    y_te = y_te.astype(int)\n",
    "    \n",
    "    # split = 20000\n",
    "    # X_tr = X_tr[:, :, :,:split]\n",
    "    # y_tr = y_tr[:split]\n",
    "    \n",
    "    print(X_tr.shape)\n",
    "    print(y_tr.shape)\n",
    "    for i in range(y_tr.shape[0]):\n",
    "        if y_tr[i] == 10:\n",
    "            y_tr[i] = 0\n",
    "    for i in range(y_te.shape[0]):\n",
    "        if y_te[i] == 10:\n",
    "            y_te[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_tr_reshaped = X_tr.reshape(X_tr.shape[3], -1)\n",
    "    X_te_reshaped = X_te.reshape(X_te.shape[3], -1)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fitting on training, transform on both training and testing\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr_reshaped)\n",
    "    X_te_scaled = scaler.transform(X_te_reshaped)\n",
    "\n",
    "    X_tr = X_tr_scaled.reshape(X_tr.shape)\n",
    "    X_te = X_te_scaled.reshape(X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # 3 color channels, 6 feature maps to output, and a 5x5 kernel. Stride is 1 for the kernel.\n",
    "        self.pool = nn.MaxPool2d(2, 2) # Pooling filter is 2x2, and a stride of 2.\n",
    "        self.conv2 = nn.Conv2d(6, 20, 5) # 6 input feature maps from conv1 + pool, 16 feature maps to output, 5x5 kernel. Stride of 1.\n",
    "        self.fc1 = nn.Linear(20*5*5, 120) # 16 for the output layer * the resulting 5x5 image after pooling\n",
    "        self.fc2 = nn.Linear(120, 84) # 120 input layer, 84 output\n",
    "        self.fc3 = nn.Linear(84, 10) # 84 input layer, 10 output (10 classes)\n",
    "\n",
    "        # Formula applied for conv = (Dimension - Filter)/Stride + 1\n",
    "\n",
    "\n",
    "    def forward(self, x, featureMap):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, featureMap*5*5) # Flattening, because Linear only takes a 1D array\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)    # Don't apply ReLU here, nor softmax. CrossEntropyLoss() applies softmax automatically, and requires raw data.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 5 Epochs\n",
      "Epoch 1/5...\n",
      "Epoch 2/5...\n",
      "Epoch 3/5...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# First, converting our data into tensors\n",
    "X_tr_transposed = X_tr.transpose((3, 2, 0, 1))\n",
    "\n",
    "X_tr_tensor = torch.from_numpy(X_tr_transposed).float().to(device)\n",
    "y_tr_tensor = torch.from_numpy(y_tr).long().to(device)\n",
    "\n",
    "# Converting our tensors into a Tensor Dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "\n",
    "# Creating a loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training the model\n",
    "n_total_steps = len(train_loader)\n",
    "epochs = list(range(5,26))\n",
    "test_error = []\n",
    "train_error = []\n",
    "for e in epochs:\n",
    "    model = ConvNet().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    print(f\"Training with {e} Epochs\")\n",
    "    for epoch in range(e):\n",
    "        print(f\"Epoch {epoch + 1}/{e}...\")\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            labels = labels.squeeze()\n",
    "            # Zeroing gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            # (batch_size, 3, 32, 32)\n",
    "            # (32, 32, 3, batch_size)\n",
    "            outputs = model.forward(inputs, 20)\n",
    "\n",
    "            # Applying loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropogation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update our weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(f\"Epoch [{epoch + 1}/{e}], Batch [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}\")\n",
    "    test_error.append(1-test(X_te, y_te, model, batch_size, 20))\n",
    "    train_error.append(1-test(X_tr, y_tr, model, batch_size, 20))\n",
    "plt.plot(epochs, test_error, label=\"Test Error\")\n",
    "plt.plot(epochs, train_error, label=\"Train Error\")\n",
    "plt.xlabel(\"Feature maps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Feature maps\")\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_te, y_te, model, batch_size, featureMap):\n",
    "  # Testing the model\n",
    "  X_te_transposed = X_te.transpose((3, 2, 0, 1))\n",
    "\n",
    "  X_te_tensor = torch.from_numpy(X_te_transposed).float().to(device)\n",
    "  y_te_tensor = torch.from_numpy(y_te).long().to(device)\n",
    "\n",
    "  # Converting our tensors into a Tensor Dataset\n",
    "  test_dataset = torch.utils.data.TensorDataset(X_te_tensor, y_te_tensor)\n",
    "\n",
    "  # Creating a loader\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  print(\"testing the model...\")\n",
    "  correct_count, all_count = 0, 0\n",
    "  for images,labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "      img = images[i].view(1, 3, 32, 32)\n",
    "      with torch.no_grad():\n",
    "          logps = model(img, featureMap)\n",
    "\n",
    "      \n",
    "      ps = torch.exp(logps)\n",
    "      probab = list(ps.cpu()[0])\n",
    "      pred_label = probab.index(max(probab))\n",
    "      true_label = labels.cpu()[i]\n",
    "      if(true_label == pred_label):\n",
    "        correct_count += 1\n",
    "      all_count += 1\n",
    "\n",
    "  print(\"Number Of Images Tested =\", all_count)\n",
    "  print(\"\\nModel Accuracy =\", (correct_count/all_count))\n",
    "  return (correct_count/all_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# First, converting our data into tensors\n",
    "X_tr_transposed = X_tr.transpose((3, 2, 0, 1))\n",
    "\n",
    "X_tr_tensor = torch.from_numpy(X_tr_transposed).float().to(device)\n",
    "y_tr_tensor = torch.from_numpy(y_tr).long().to(device)\n",
    "\n",
    "# Converting our tensors into a Tensor Dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "\n",
    "# Creating a loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "model = ConvNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.conv2 = nn.Conv2d(6, 5, 5)\n",
    "model.fc1 = nn.Linear(5*5*5, 120)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        labels = labels.squeeze()\n",
    "        # Zeroing gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # (batch_size, 3, 32, 32)\n",
    "        # (32, 32, 3, batch_size)\n",
    "        outputs = model.forward(inputs, 5)\n",
    "\n",
    "        # Applying loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropogation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update our weights\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(X_te, y_te, model, batch_size, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changed epoch from 5 to 10, accuracy went from 64 to 83"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
